{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["1. Segment each video into shots\n","2. Extract features from each shot, including low-level features (such as color and motion) and high-level semantic features (such as object detection and scene recognition)\n","3. Cluster the shots based on their feature representations to group similar shots together\n","4. Identify key frames within each cluster by selecting frames that best represent the cluster\n","5. Order the key frames based on their importance score, which is calculated using a combination of visual and semantic information\n","6. Generate a summary by selecting the top-ranked key frames"],"metadata":{"id":"vzL1CCbVcbub"}},{"cell_type":"markdown","source":["## Step1"],"metadata":{"id":"BbQMQBSdcm0J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ai__b6_UpGSl"},"outputs":[],"source":["def extract_shots(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","    # Group frames into shots with fixed interval of 30 frames\n","    shots = []\n","    start_frame = 0\n","    while True:\n","        end_frame = start_frame + 29\n","        if end_frame >= cap.get(cv2.CAP_PROP_FRAME_COUNT):\n","            end_frame = cap.get(cv2.CAP_PROP_FRAME_COUNT) - 1\n","        shots.append((int(start_frame), int(end_frame)))\n","        if end_frame >= cap.get(cv2.CAP_PROP_FRAME_COUNT) - 1:\n","            break\n","        start_frame = end_frame + 1\n","\n","    cap.release()\n","\n","    return shots[:-1]\n"]},{"cell_type":"markdown","source":["## step 2"],"metadata":{"id":"6HiLAfTQfXVp"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","def compute_optical_flow(prev_gray, curr_gray, prev_pts):\n","    curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None)\n","\n","    # Filter out points for which the flow is not found\n","    good_pts = prev_pts[status==1]\n","    new_pts = curr_pts[status==1]\n","\n","    # Find the average flow vector\n","    flow_vec = np.mean(new_pts - good_pts, axis=0)\n","\n","    return flow_vec"],"metadata":{"id":"Dw1Of4CwqNcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","def extract_features(video_path, shots):\n","    cap = cv2.VideoCapture(video_path)\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","    features = []\n","\n","    for shot in shots:\n","        start_frame, end_frame = shot\n","        frame_count = end_frame - start_frame + 1\n","\n","        # Skip shots with less than two frames\n","        if frame_count < 2:\n","            continue\n","\n","        # Initialize feature vectors\n","        color_features = np.zeros((frame_count, 3), dtype=np.float32)\n","        motion_features = np.zeros((frame_count-1, 2), dtype=np.float32)\n","\n","        # Read first frame of shot\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","        ret, prev_frame = cap.read()\n","        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","\n","        # Compute color feature for first frame\n","        color_features[0] = cv2.mean(prev_frame)[:3]\n","\n","        # Initialize feature points for Lucas-Kanade method\n","        prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=100, qualityLevel=0.01, minDistance=10, blockSize=3)\n","\n","        # Iterate over frames in shot\n","        for i in range(1, frame_count):\n","            # Read current frame\n","            ret, curr_frame = cap.read()\n","            if not ret:\n","                break\n","            curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n","\n","            # Compute color feature\n","            color_features[i] = cv2.mean(curr_frame)[:3]\n","\n","            # Compute motion feature using Lucas-Kanade method\n","            flow_vec = compute_optical_flow(prev_gray, curr_gray, prev_pts)\n","            motion_features[i-1, 0] = np.linalg.norm(flow_vec)\n","            motion_features[i-1, 1] = np.arctan2(flow_vec[1], flow_vec[0])\n","\n","            prev_gray = curr_gray\n","            prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=100, qualityLevel=0.01, minDistance=10, blockSize=3)\n","\n","        # Normalize motion features\n","        motion_features /= np.max(motion_features, axis=0)\n","\n","        # Combine color and motion features\n","        shot_features = np.hstack((color_features[:-1], motion_features))\n","\n","        # Add shot features to list of features\n","        features.append(shot_features)\n","\n","    cap.release()\n","\n","    return features\n"],"metadata":{"id":"FffWTYgafYjW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## step 3"],"metadata":{"id":"JlRfUfe4fctJ"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","def cluster_shots(features, num_clusters):\n","    stacked_features = np.vstack(features)\n","    kmeans = KMeans(n_clusters=num_clusters)\n","    kmeans.fit(stacked_features)\n","    labels = kmeans.labels_\n","\n","    clusters = [[] for _ in range(num_clusters)]\n","    current_shot_index = 0\n","    for i, label in enumerate(labels):\n","        while i >= sum(len(c) for c in clusters[:label+1]):\n","            current_shot_index += 1\n","        clusters[label].append(current_shot_index)\n","\n","    return clusters\n"],"metadata":{"id":"AHIf18nKfd57"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import MiniBatchKMeans\n","\n","def cluster_shots(features, num_clusters):\n","    stacked_features = np.vstack(features)\n","    kmeans = MiniBatchKMeans(n_clusters=num_clusters)\n","    kmeans.fit(stacked_features)\n","    labels = kmeans.labels_\n","\n","    clusters = [[] for _ in range(num_clusters)]\n","    current_shot_index = 0\n","    for i, label in enumerate(labels):\n","        while i >= sum(len(c) for c in clusters[:label+1]):\n","            current_shot_index += 1\n","        clusters[label].append(current_shot_index)\n","\n","    return clusters\n"],"metadata":{"id":"QF573NGG7wQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# create a 2D array\n","a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","\n","# flatten the array column-wise\n","b = a.flatten(order='F')\n","\n","print(a)\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_aSoH1Om9IF","executionInfo":{"status":"ok","timestamp":1681473169872,"user_tz":-330,"elapsed":6,"user":{"displayName":"Surya Teja Kolli","userId":"01370602188602045407"}},"outputId":"6ccb7b8c-24a4-4c4d-ffab-30eead577ad5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 2 3]\n"," [4 5 6]\n"," [7 8 9]]\n","[1 4 7 2 5 8 3 6 9]\n"]}]},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","\n","def cluster_shots(features, num_clusters):\n","    # Reduce the dimensionality of the feature vectors using PCA\n","    stacked_features = np.vstack(features)\n","    pca = PCA(n_components=2)\n","    features_pca = pca.fit_transform(stacked_features)\n","    \n","    # Perform clustering on the reduced feature vectors\n","    kmeans = KMeans(n_clusters=num_clusters)\n","    kmeans.fit(features_pca)\n","    labels = kmeans.labels_\n","\n","    clusters = [[] for _ in range(num_clusters)]\n","    current_shot_index = 0\n","    for i, label in enumerate(labels):\n","        while i >= sum(len(c) for c in clusters[:label+1]):\n","            current_shot_index += 1\n","        clusters[label].append(current_shot_index)\n","\n","    return clusters"],"metadata":{"id":"AiKfgLbADH5O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## step 4"],"metadata":{"id":"GneZZwu6gaex"}},{"cell_type":"code","source":["def identify_keyframes(clusters, features):\n","    keyframes = []\n","\n","    for cluster in clusters:\n","        # Find representative shot for cluster (shot with smallest Euclidean distance to cluster centroid)\n","        cluster_features = np.array([features[i] for i in cluster])\n","        centroid = np.mean(cluster_features, axis=0)\n","        distances = np.linalg.norm(cluster_features - centroid, axis=1)\n","        representative_shot = cluster[np.argmin(distances)]\n","\n","        # Add keyframe for representative shot (first frame of representative shot)\n","        keyframes.append(representative_shot[0])\n","\n","    return keyframes\n"],"metadata":{"id":"gvPakE_ugcVx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## step 5"],"metadata":{"id":"dNBcZLYmgcyq"}},{"cell_type":"code","source":["def calculate_importance_score(keyframes, features, labels, num_clusters, semantic_scores):\n","    # Compute visual scores (based on color and motion features)\n","    color_features = np.array([features[i][:3] for i in keyframes])\n","    motion_features = np.array([features[i][3:] for i in keyframes])\n","    color_scores = np.linalg.norm(color_features, axis=1)\n","    motion_scores = np.linalg.norm(motion_features, axis=1)\n","    visual_scores = color_scores + motion_scores\n","\n","    # Compute semantic scores (based on cluster labels and semantic scores)\n","    cluster_sizes = [len(cluster) for cluster in labels]\n","    cluster_weights = np.array(cluster_sizes) / np.sum(cluster_sizes)\n","    semantic_scores = np.array([semantic_scores[i] for i in labels])\n","    semantic_scores_weighted = np.sum(semantic_scores * cluster_weights, axis=1)\n","    semantic_scores_normalized = (semantic_scores_weighted - np.min(semantic_scores_weighted)) / (np.max(semantic_scores_weighted) - np.min(semantic_scores_weighted))\n","    semantic_scores_normalized = np.clip(semantic_scores_normalized, 0, 1)\n","\n","    # Combine visual and semantic scores to get importance scores\n","    importance_scores = 0.5 * visual_scores + 0.5 * semantic_scores_normalized\n","\n","    # Sort keyframes by importance score\n","    sorted_indices = np.argsort(importance_scores)[::-1]\n","    sorted_keyframes = [keyframes[i] for i in sorted_indices]\n","\n","    return sorted_keyframes"],"metadata":{"id":"VBisvRKwgfM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## step 6"],"metadata":{"id":"ClS5Vdamgf7H"}},{"cell_type":"code","source":["def generate_summary(keyframes, num_frames):\n","    # Sort keyframes by index (to ensure temporal order)\n","    keyframes = sorted(keyframes)\n","\n","    # Select top-ranked keyframes based on their index\n","    summary = keyframes[:num_frames]\n","\n","    return summary"],"metadata":{"id":"0msBOK3lghbT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## implementation"],"metadata":{"id":"Eo21eGe_QphU"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# Step 1: Segment the video into shots\n","video_file = r'Air_Force_One.mp4'\n","shots = extract_shots(video_file)\n","\n","# Step 2: Extract features from each shot (color and motion)\n","features = extract_features(video_file,shots)\n","\n","# Step 3: Cluster shots based on their feature representations\n","num_clusters = 100\n","clusters = cluster_shots(features, num_clusters)\n","\n","# Step 4: Identify keyframes within each cluster\n","keyframes = identify_keyframes(clusters, features)\n","\n","# Step 6: Generate summary by selecting top-ranked keyframes\n","num_frames = 150\n","summary = generate_summary(keyframes, num_frames)\n","\n","# Load video using OpenCV\n","cap = cv2.VideoCapture(video_file)\n","\n","# Extract frames corresponding to summary frames and save as a new video\n","out_file = 'summary.mp4'\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","out = cv2.VideoWriter(out_file, fourcc, 30.0, (640, 360)) # Change resolution as needed\n","\n","for frame_idx in summary:\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n","    ret, frame = cap.read()\n","    out.write(frame)\n","\n","cap.release()\n","out.release()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"id":"ctSXZOCCQpN0","outputId":"9034b3bf-5cc1-4379-ece2-d7ad2d7e7c95","executionInfo":{"status":"error","timestamp":1680955167259,"user_tz":-330,"elapsed":47292,"user":{"displayName":"Surya Teja Kolli","userId":"01370602188602045407"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-1244a5932895>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Step 3: Cluster shots based on their feature representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnum_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_shots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Step 4: Identify keyframes within each cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-195f1bea1d36>\u001b[0m in \u001b[0;36mcluster_shots\u001b[0;34m(features, num_clusters)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcurrent_shot_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mcurrent_shot_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_shot_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(np.shape(np.vstack(features)))"],"metadata":{"id":"4eQ358RPjOGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"idzof6cBjVXC"},"execution_count":null,"outputs":[]}]}